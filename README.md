# Back-propagation
implementation of back-propagation algorithm from scratch with both sigmoid and tanh activation functions.

Implementation of the Back-Propagation learning algorithm on a multi-layer neural networks, which can be able to classify a stream of input data to one of a set of predefined classes, using the dry beans data in both the training and testing processes. (Each class has 50 samples: trained NN with the first 30 non-repeated samples, and tested it with the remaining 20 samples)


 User Input:
• Number of hidden layers
• Number of neurons in each hidden layer
• Learning rate (eta)
• Number of epochs (m)
• Add bias or not (Checkbox)
• Choose to use Sigmoid or Hyperbolic Tangent sigmoid as the activation function

Output:
• Confusion matrix
• Overall Accuracy

